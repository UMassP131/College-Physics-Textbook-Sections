<?xml version="1.0"?>
<document xmlns="http://cnx.rice.edu/cnxml" id="imported-from-openoffice" module-id="imported-from-openoffice" cnxml-version="0.7">
  <title>A way to think about entropy - sharing</title>
<metadata xmlns:md="http://cnx.rice.edu/mdml" mdml-version="0.5">
  <!-- WARNING! The 'metadata' section is read only. Do not edit below.
       Changes to the metadata section in the source will not be saved. -->
  <md:repository>https://legacy.cnx.org/content</md:repository>
  <md:content-id>new</md:content-id>
  <md:title>A way to think about entropy - sharing</md:title>
  <md:version>**new**</md:version>
  <md:created>2017/07/20 13:56:40.379 GMT-5</md:created>
  <md:revised>2017/07/20 13:56:40.581 GMT-5</md:revised>
  <md:actors>
    <md:person userid="davidnvn">
      <md:firstname>David</md:firstname>
      <md:surname>Nguyen</md:surname>
      <md:fullname>David Nguyen</md:fullname>
      <md:email>dnguyen@umass.edu</md:email>
    </md:person>
  </md:actors>
  <md:roles>
    <md:role type="author">davidnvn</md:role>
    <md:role type="maintainer">davidnvn</md:role>
    <md:role type="licensor">davidnvn</md:role>
  </md:roles>
  <md:license url="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License 4.0</md:license>
  <!-- For information on license requirements for use or modification, see license url in the
       above <md:license> element.
       For information on formatting required attribution, see the URL:
         CONTENT_URL/content_info#cnx_cite_header
       where CONTENT_URL is the value provided above in the <md:content-url> element.
  -->
  <md:keywordlist>
    <md:keyword>umassphysics</md:keyword>
  </md:keywordlist>
  <md:abstract/>
  <md:language>en</md:language>
  <!-- WARNING! The 'metadata' section is read only. Do not edit above.
       Changes to the metadata section in the source will not be saved. -->
</metadata>

<content>

<note id="copyright">The following is based off of <newline /> umdberg / A way to think about entropy -- sharing. Available at: <link url="http://umdberg.pbworks.com/w/page/50323410/A%20way%20to%20think%20about%20entropy%20--%20sharing. (Accessed: 20th July 2017)
">http://umdberg.pbworks.com/w/page/50323410/A%20way%20to%20think%20about%20entropy%20--%20sharing.</link>

</note>

    <para id="import-auto-idm780001872">We've now read a lot about the second law of thermodynamics and the idea of <emphasis effect="italics">entropy. </emphasis>The basic idea was that we are looking coarsely at a system that has a fine-grained structure that is changing rapidly in a random way. Specifically, we are looking at things <emphasis effect="italics">macroscopically</emphasis> -- and by this we mean at a level at which the structure of matter in terms of molecules and atoms can't be seen. We only care about average properties of the molecules; things like temperature, pressure<emphasis effect="italics">, </emphasis>and concentration. It is generally useful to ignore the fact that the molecules are actually moving around chaotically, colliding with each other, and chemical reactions are happening (and unhappening).</para>
    <para id="import-auto-idm798723216">What we <emphasis effect="italics">are</emphasis> interested in, is the following:</para>
    <para id="import-auto-idm234749712">
      <emphasis effect="italics">If two parts of a system we are considering are NOT in thermodynamic equilibrium, what will naturally tend to happen?</emphasis>
    </para>
    <para id="import-auto-idm886438512">This is the question that the second law of thermodynamics gives the answer to. It tells us that if one part of a system is hotter than another, the natural spontaneous tendency of the system is for the temperature to even out. If a chemical reaction can occur, the reaction will continue in one direction until the rate of the reverse reaction is equal to the rate of the reaction. When the rates of forward and reverse reactions are equal, the amount of each chemical stays the same and it is called chemical equilibrium.</para>
    <para id="import-auto-idm1008452800">To understand these situations in general and to figure out which way things will happen under what conditions, we introduced the concept of entropy and the second law of thermodynamics. The core idea is that to each coarse-grained view of a particular system (its pressure, temperature, chemical concentrations, etc. -- its <emphasis effect="italics">macrostate</emphasis>) there are many, many different possible possible arrangements and motions of the individual molecules (its <emphasis effect="italics">microstate</emphasis>). The idea of the second law is:</para>
    <para id="import-auto-idm811191744">
      <emphasis effect="italics">A system that is not in thermodynamic equilibrium will spontaneously go towards the state with the largest number of microstates.</emphasis>
    </para>
    <para id="import-auto-idm792740960">The reason for this is that we assume that as the system goes through its various chaotic states, each microstate is equally probable. Therefore, the system will most often wind up in the macrostate that corresponds to the largest number of microstates.</para>
    <para id="import-auto-idm176650784">Since the <emphasis effect="italics">entropy</emphasis> is defined as (a constant times) the logarithm of the number of microstates, <emphasis effect="italics">S = k</emphasis><emphasis effect="italics">B</emphasis><emphasis effect="italics"> ln(W), </emphasis>(see <link url="http://umdberg.pbworks.com/w/page/49691685/Why%20entropy%20is%20logarithmic"><emphasis effect="underline">Why entropy is logarithmic</emphasis></link>), the second law can be restated as</para>
    <para id="import-auto-idm2147337296">
      <emphasis effect="italics">Systems that are not in thermodynamic equilibrium will spontaneously transform so as to increase the entropy.</emphasis>
    </para>
    <para id="import-auto-idm829481712">Well. This is an impressive sounding statement. But what does it mean? It's pretty plausible to think about flipping coins and deciding whether 5 heads and 5 tails is more or less likely to happen than tossing 10 heads in a row. But how does counting microstates help us see that hot and cold objects placed together will tend to go to a common temperature? You can do it, but it takes a LOT of heavy mathematical lifting -- and doesn't particularly help us conceptually. Another way to think about it that might help, is to think of entropy as a measure of <emphasis effect="italics">sharing</emphasis>.</para>
    <section id="import-auto-idm2302596240">
      <title>If energy is uniformly spread, it's useless.</title>
      <para id="import-auto-idm788122800">Thermodynamic equilibrium means that the energy in a system is uniformly spread among all the degrees of freedom (i.e. distributed evenly among all places energy can go, for example, for each molecule among both its potential and kinetic energies). In such a state, the energy no longer "flows" from one set of molecules to another or from one kind of energy to another. Thus in thermodynamic equilibrium the energy in a system is useless; no work, either physical or chemical can be done. If we want to think about how useful some energy is, we need to know not just how much energy we have, but how it is distributed. The further from equilibrium it is, the more useful it will be. We are working towards developing the idea of not just energy, but <emphasis effect="italics">free energy</emphasis> -- useful energy.</para>
      <para id="import-auto-idm170940512">In some sense, entropy is a measure of how uniformly the energy is distributed in a system. If the system is fully at thermodynamic equilibrium the entropy is a maximum. If the entropy is lower than than maximum, then there is room for the entropy to go up as the system moves towards thermodynamic equilibrium. The system will spontaneously and naturally be redistributing its energy toward the equilibrium state. During such a redistribution, work can get done and an organism can make a living.</para>
      <para id="import-auto-idm2249221360">Joe Redish 1/29/12<emphasis effect="italics"> </emphasis></para>
    </section>
  </content>
</document>
